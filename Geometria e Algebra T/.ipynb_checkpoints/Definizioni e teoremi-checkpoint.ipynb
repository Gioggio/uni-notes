{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spazi vettoriali\n",
    "\n",
    "**Definizione** Siano $v_1,v_2,...,v_s\\in V$ vettori. Si dice **sottospazio vettoriale generato** da $v_1,...,v_s$ (o anche **chiusura lineare** di $v_1,...,v_s$) l'insieme di tutte le combinazioni lineari di $v_1,...,v_s$, e si scrive\n",
    "\n",
    "$$< v_1,v_2,...,v_s >$$\n",
    "\n",
    "In altri termini:\n",
    "\n",
    "$$< v_1,v_2,...,v_s >=\\{a_1v_1+...+a_sv_s\\mid a_1,...,a_s\\in \\mathbb{K}\\}$$\n",
    "\n",
    "**Osservazione** La chiusura lineare di $v_1,...,v_s$ è sempre un sottospazio di $V$!\n",
    "\n",
    "**Proposizione** La chiusura lineare è anche il più piccolo sottospazio vettoriale di $V$ contenente $v_1,...,v_s$.\n",
    "\n",
    "**Proposizione** Siano $U$ e $W$ due sottospazi vettoriali di $V$. $\\bf{U \\cap W}$ è un sottospazio vettoriale di $V$\n",
    "\n",
    "**Dimostrazione** Siano $v_1,v_2\\in U\\cap W$. Allora visto che $U$ e $W$ sono spazi vettoriali, $v_1+v_2\\in U$ e $v_1+v_2 \\in W$. Quindi $v_1+v_2\\in U\\cap W$. Inoltre $\\forall\\lambda\\in\\mathbb{R}$, $v\\in U$ e $v\\in W$ si ha che $\\lambda v\\in U$ e $\\lambda v\\in W$. Quindi $\\lambda v\\in U\\cap W$.\n",
    "\n",
    "**Osservazione** In generale $U\\cup W$ non è un sottospazio di V!\n",
    "\n",
    "**Definizione** Siano $U$ e $W$ due sottospazi vettoriali di $V$. Chiamiamo somma di $U$ e $W$ l'insieme\n",
    "\n",
    "$$U+W=\\{u+w\\mid u\\in U,w\\in W\\}$$\n",
    "\n",
    "**Proposizione** $U+W$ è un sottospazio vettoriale di $V$.\n",
    "\n",
    "**Dimostrazione** Presi $u_1+w_1\\in U+W$, $u_2+w_2\\in U+W$.\n",
    "\n",
    "$$(u_1+w_1)+(u_2+w_2)=u_1+w_1+u_2+w_2=(u_1+u_2)+(w_1+w_2)$$\n",
    "\n",
    "Siccome $(u_1+u_2)\\in U$ e $(w_1+w_2)\\in W$, allora $(u_1+u_2)+(w_1+w_2)\\in U+W$.\n",
    "\n",
    "Presi $u\\in U$, $w\\in W$ e $\\lambda\\in\\mathbb{R}$..\n",
    "\n",
    "$$\\lambda(u+w)=\\lambda u+\\lambda w\\in U+W$$\n",
    "\n",
    "Siccome $\\lambda u\\in U$ e $\\lambda w\\in W$, allora $\\lambda u+\\lambda w\\in U+W$.\n",
    "\n",
    "**Teorema** (Relazione di Grassman) Siano $V_1$ e $V_2$ spazi vettoriali finitamente generati. Allora\n",
    "\n",
    "$$\\text{dim}(V_1+V_2)=\\text{dim}V_1+\\text{dim}V_2-\\text{dim}V_1\\cap V_2$$\n",
    "\n",
    "**Dimostrazione** Prendiamo una base $B=\\{v_1,...,v_s\\}$ di $V_1\\cap V_2$. Completiamo $B$ a una base $B_{V_1}$ di $V_1$ e a una base $B_{V_2}$ di $V_2$.\n",
    "\n",
    "$$B_{V_1}=\\{v_1,...,v_s,w_{s+1},...,w_{s+r}\\}$$\n",
    "\n",
    "$$B_{V_2}=\\{v_1,...,v_s,w'_{s+1},...,w'_{s+t}\\}$$\n",
    "\n",
    "Dobbiamo ora dimostrare che $S=\\{v_1,...,v_s,w_{s+1},...,w_{s+r},w'_{s+1},...,w'_{s+t}\\}$ è una base per $V_1+V_2$\n",
    "\n",
    "1. Osserviamo prima di tutto che $S=\\{v_1,...,v_s,w_{s+1},...,w_{s+r},w'_{s+1},...,w'_{s+t}\\}$ è un sistema di generatori per $V_1+V_2$. Infatti, qualunque combinazione lineare di questi vettori ci dà un vettore di $V_1+V_2$, ovvero $< S >\\subseteq V_1+V_2$. Inoltre, è anche vero che $V_1+V_2\\subseteq < S >$. Quindi $V_1+V_2=S$.\n",
    "\n",
    "2. Dimostriamo che i vettori di $S$ sono linearmente indipendenti. Consideriamo la seguente uguaglianza:\n",
    "\n",
    "$$a_1v_1+...+a_sv_s+b_{s+1}w_{s+1}+...+b_{s+r}w_{s+r}+c_{s+1}w'_{s+1}+...+c_{s+t}w'_{s+t}=0_V$$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Ricaviamo\n",
    "\n",
    "$$a_1v_1+...+a_sv_s+b_{s+1}w_{s+1}+...+b_{s+r}w_{s+r}=-c_{s+1}w'_{s+1}-...-c_{s+t}w'_{s+t}$$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Sappiamo che il vettore $\\overline v$ a sinistra appartiene a $V_1$, mentre quello a destra appartiene a $V_2$. Quindi entrambi appartengono a $V_1\\cap V_2$. Inoltre, sia ha necessariemente $c_{s+1}=...=c_{s+t}=0$, altrimenti il vettore a destra potrebbe essere scritto come combinazione lineare dei vettori $v_1,...,v_s$; ciò però non può accadere perché i vettori $w'_{s+1},...,w'_{s+t}$ e $v_1,...,v_s$ sono linearmente indipendenti (formano una base di $V_2$).\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Analogamente si può dimostrare che necessariamente $b_{s+1}=...=b_{s+r}=0$. Si ottiene quindi l'equazione\n",
    "\n",
    "$$a_1v_1+...+a_sv_s=0_V$$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Quindi anche $a_1=...=a_s=0$. Perciò i vettori di $S$ sono linearmente indipendenti.\n",
    "\n",
    "Perciò $S$ è una base. Quindi $\\text{dim}V_1+V_2=s+r+t$, $\\text{dim}V_1=s+r$, $\\text{dim}V_2=s+t$ e $\\text{dim}V_1\\cap V_2=s$, soddisfando la tesi.\n",
    "\n",
    "**Osservazione** Quando $V_1\\cap V_2=\\{0_V\\}$, cioè $\\text{dim}V_1\\cap V_2=0$, si dice **somma diretta** e si scrive $V_1\\oplus V_2$. La Relazione di Grassman diventa\n",
    "\n",
    "$$\\text{dim}V_1\\oplus V_2=\\text{dim}V_1+\\text{dim}V_2$$\n",
    "\n",
    "# Matrici\n",
    "\n",
    "$M_{m\\times n}(\\mathbb{K})$= insieme delle matrici $m\\times n$ ($m$ righe e $n$ colonne) a coefficienti nel campo $\\mathbb{K}$.\n",
    "\n",
    "**Definizione** Siano $A,B\\in M_{m\\times n}(\\mathbb{K})$, $A=(a_i^j)$, $B=(b_i^j)$ e $\\lambda\\in\\mathbb{K}$. Si definiscono\n",
    "\n",
    "1. $A+B=(a_i^j+b_i^j)$\n",
    "2. $\\lambda A=(\\lambda a_i^j)$\n",
    "\n",
    "**Proposizione** $(M_{m\\times n},+)(\\mathbb{K})$ è un gruppo commutativo.\n",
    "\n",
    "**Proposizione** $(M_{m\\times n},+,\\cdot)(\\mathbb{K})$ è un gruppo commutativo.\n",
    "\n",
    "**Proprietà**\n",
    "\n",
    "1. $\\alpha(A+B)=\\alpha A+\\alpha B$\n",
    "2. $(\\alpha + \\beta)A=\\alpha A + \\beta A$\n",
    "3. $(\\alpha\\beta)A=\\alpha(\\beta A)$\n",
    "4. $1A=A$\n",
    "\n",
    "**Definizione** Si definiscono **matrici quadrate** le matrici appartenenti a $M_{n\\times n}(\\mathbb{K})$.\n",
    "\n",
    "**Definizione** Si definiscono **matrici triangolari alte** le matrici quadrate i cui elementi sotto la diagonale principale sono nulli.\n",
    "\n",
    "**Definizione** Si definiscono **matrici triangolari basse** le matrici quadrate i cui elementi sopra la diagonale principale sono nulli.\n",
    "\n",
    "**Definizione** Si definiscono **matrici diagonali** le matrici contemporaneamente triangolari alte e triangolari basse.\n",
    "\n",
    "**Definizione** Sia $A=(a_i^j)$ una matrice. Si definisce **matrice trasposta** la matrice $^tA=(b_i^j)$, dove $b_i^j=a_j^i$.\n",
    "\n",
    "**Definizione** Si definiscono **matrici simmetriche** le matrici tali che $A=^tA$.\n",
    "\n",
    "**Definizione** Si definiscono **matrici antisimmetriche** le matrici tali che $A=-^tA$.\n",
    "\n",
    "**Definizione** Lo **spazio delle righe** di una matrice $m\\times n$ è lo spazio vettoriale delle righe della matrice in $\\mathbb{K}^n$.\n",
    "\n",
    "**Definizione** Lo **spazio delle colonne** di una matrice $m\\times n$ è lo spazio vettoriale delle colonne della matrice in $\\mathbb{K}^n$.\n",
    "\n",
    "**Teorema** Ogni operazione riga conserva la lineare dipendenza (o indipendenza) delle colonne della matrice.\n",
    "\n",
    "**Dimostrazione** Sia $A$ la seguente matrice\n",
    "\n",
    "$$A=\\begin{bmatrix}a_1^1 & ... & a_1^n\\\\ ... & ... & ...\\\\ a_m^1 & ... & a_m^n\\end{bmatrix}$$\n",
    "\n",
    "Chiamiamo $c_1,...,c_n$ le colonne della matrice.\n",
    "\n",
    "$$\\alpha_1c_1+...+\\alpha_nc_n=0\\iff \\alpha_1\\begin{bmatrix}a_1^1\\\\ ...\\\\ a_m^1\\end{bmatrix}+...+\\alpha_n\\begin{bmatrix}a_1^n\\\\ ...\\\\ a_m^n\\end{bmatrix}=0$$\n",
    "\n",
    "Possiamo riscriverlo come\n",
    "\n",
    "$$\\begin{bmatrix}\\alpha_1a_1^1 + \\alpha_2a_1^2 + ... + \\alpha_na_1^n\\\\ ...\\\\ \\alpha_1a_m^1 + \\alpha_2a_m^2 + ... + \\alpha_na_m^n \\end{bmatrix}=\\begin{bmatrix}0\\\\ ...\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Che equivale al sistema\n",
    "\n",
    "$$\\left\\{\\begin{matrix}\\alpha_1a_1^1 + \\alpha_2a_1^2 + ... + \\alpha_na_1^n=0\\\\ ...\\\\ \\alpha_1a_m^1 + \\alpha_2a_m^2 + ... + \\alpha_na_m^n=0\\end{matrix}\\right.$$\n",
    "\n",
    "Osserviamo che compiendo operazioni riga sulle equazioni del sistema, quest'ultimo cambia, ma non cambiano le soluzioni.\n",
    "\n",
    "**Definizione** Si dice **rango** di una matrice $A\\in M_{m\\times n}(\\mathbb{K})$ il numero di pivot di una sua qualsiasi forma ridotta. Si scrive $\\text{r}(A)$, $\\text{rank}(A)$ o $\\rho(A)$.\n",
    "\n",
    "**Proposizione** $\\text{r}(A)=\\text{r}(^tA)$\n",
    "\n",
    "**Dimostrazione** Osserviamo che il rango si può anche definire equivalentemente come:\n",
    "\n",
    "1. la dimensione dello spazio delle righe\n",
    "2. la dimensione dello spazzio delle colonne\n",
    "\n",
    "Quando si compiono operazioni riga:\n",
    "\n",
    "1. si preserva lo spazio delle righe e dunque la sua dimensione\n",
    "2. non si preserva lo spazio delle colonne, ma la proprietà di lineare dipendenza (o indipendenza). Perciò lo spazio cambia, ma non cambia la cardinalità della base del nuovo spazio, ovvero la dimensione\n",
    "\n",
    "Applico trasformazioni riga alla matrice fino ad arrivare alla forma completamente ridotta. Per vedere la relazione tra la dimensione dello spazio delle righe e dello spazio delle colonne si può fare riferimento alla forma completamente ridotta.\n",
    "\n",
    "1. La dimensione dello spazio delle righe coincide con il numero di pivot.\n",
    "2. La dimensione dello spazio delle colonne coincide con il numero di pivot, perché lo spazio delle colonne è generato dalle colonne passanti per i pivot. Quindi le dimensioni degli spazi delle righe e delle colonne coincidono.\n",
    "\n",
    "**Definizioni** Siano $A,B$ matrici. Esse si dicono **conformabili** se il numero di colonne della prima è uguale al numero di righe della seconda.\n",
    "\n",
    "**Definizione** Siano $A=(a_i^j)\\in M_{m\\times r}(\\mathbb{K})$ e $B=(b_i^j)\\in M_{r\\times n}(\\mathbb{K})$. Si definisce prodotto di matrici\n",
    "\n",
    "$$AB=(\\sum_{k=1}^n{a_i^kb_k^j})$$\n",
    "\n",
    "**Proprietà**\n",
    "\n",
    "1. $(AB)C=A(BC)$\n",
    "2. $(A_1+A_2)B=A_1B+A_2B$ e $A(B_1+B_2)=AB_1+AB_2$\n",
    "3. $\\alpha(AB)=(\\alpha A)B=A(\\alpha B)$\n",
    "4. $0_{m\\times r}B_{r\\times n}=0_{m\\times n}$ e $A_{m\\times r}0_{r\\times n}=0_{m\\times n}$\n",
    "\n",
    "**Proposizione** $(M_{n\\times n},+,\\cdot)$ è un anello non commutativo. La **matrice identità** $I_n=\\begin{bmatrix}1 & 0 & ... & 0\\\\ 0 & 1 & ... & 0\\\\ 0 & 0 & ... & 0\\\\ 0 & 0 & ... & 1\\end{bmatrix}$ è l'unità dell'anello.\n",
    "\n",
    "**Definizione** Sia $A\\in M_{m\\times n}(\\mathbb{K})$. Si definisce **matrice inversa** di $A$, indicata con $A^{-1}$, quella matrice, se esiste, tale che $AA^{-1}=A^{-1}A=I_{n}$.\n",
    "\n",
    "**Osservazione** La matrice inversa, se esiste, esiste solo per matrici quadrate.\n",
    "\n",
    "**Dimostrazione** Supponiamo $A\\in M_{m\\times n}(\\mathbb{K})$. Siccome per definizione di matrice inversa si deve avere $AA^{-1}=A^{-1}A$ allora $A^{-1}$ dovrebbe essere contemporaneamente una matrice $n\\times t$ e $t\\times m$.\n",
    "\n",
    "**Proposizione** Se esiste la matrice inversa di A, allora è unica.\n",
    "\n",
    "**Dimostrazione** Supponiamo $A'$ e $A''$ inverse di $A$. Allora\n",
    "\n",
    "$$A'AA''=\\left\\{\\begin{matrix}(A'A)A''=A''\\\\ A'(AA'')=A'\\end{matrix}\\right.\\implies A'=A''$$\n",
    "\n",
    "**Definizione** Le matrici che non ammettono inversa si dicono **non invertibili**. Quelle che la ammettono si dicono **singolari**.\n",
    "\n",
    "**Definizione** Sia $A\\in M_{2\\times 2}(\\mathbb{K})$ della forma\n",
    "\n",
    "$$A=\\begin{bmatrix}a & b\\\\ c & d\\end{bmatrix}$$\n",
    "\n",
    "$ad-bc$ si chiama **determinante** di una matrice $2\\times 2$ e si indica con $\\text{det}A$.\n",
    "\n",
    "**Osservazione** Sia $A\\in M_{2\\times 2}(\\mathbb{K})$ della forma\n",
    "\n",
    "$$A=\\begin{bmatrix}a & b\\\\ c & d\\end{bmatrix}$$\n",
    "\n",
    "L'inversa di $A$ è:\n",
    "\n",
    "$$A^{-1}=\\frac{1}{\\text{det}A}\\begin{bmatrix}d & -b\\\\ -c & a\\end{bmatrix}=\\begin{bmatrix}\\frac{d}{\\text{det}A} & \\frac{-b}{\\text{det}A}\\\\ \\frac{-c}{\\text{det}A} & \\frac{a}{\\text{det}A}\\end{bmatrix}$$\n",
    "\n",
    "**Dimostrazione** Si dimostra eguagliando i prodotti matriciali $AA^{-1}$ e $A^{-1}A$.\n",
    "\n",
    "**Osservazione** Se $\\text{det}A=ad-bc=0$ l'inversa di $A$ non esiste.\n",
    "\n",
    "**Proposizione** Siano $A,B\\in M_{n\\times n}(\\mathbb{K})$ invertibili. Allora\n",
    "\n",
    "$$(AB)^{-1}=B^{-1}A^{-1}$$\n",
    "\n",
    "**Dimostrazione**\n",
    "\n",
    "$$(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AI_nA^{-1}=AA^{-1}=I_n$$\n",
    "\n",
    "**Proposizione** Siano $A,B$ matrici conformabili. Allora\n",
    "\n",
    "$$^t(AB)=^tB^tA$$\n",
    "\n",
    "**Dimostrazione** Siano $A=(a_i^j)$ e $B=(b_i^j)$.\n",
    "\n",
    "$$AB=(\\sum_{k=1}^n{a_i^kb_k^j})$$\n",
    "\n",
    "$$^t(AB)=(\\sum_{k=1}^n{a_k^ib_j^k})$$\n",
    "\n",
    "$$^tA=(c_i^j)=(a_j^i)\\ e\\ ^tB=(d_i^j)=(b_j^i)$$\n",
    "\n",
    "$$^tB^tA=(\\sum_{k=1}^n{d_i^kc_k^j})=(\\sum_{k=1}^n{b_j^ka_k^i})$$\n",
    "\n",
    "**Proposizione** Siano $A,B\\in M_{m\\times n}(\\mathbb{K})$. Allora\n",
    "\n",
    "$$^t(A+B)=^tA+^tB$$\n",
    "\n",
    "**Proposizione** Sia $A\\in M_{n\\times n}(\\mathbb{K})$ e $k\\in\\mathbb{N}$.\n",
    "\n",
    "1. $A^0=I_n$\n",
    "2. $A^1=A$\n",
    "3. $A^k=A\\cdot A\\cdot A\\cdot ...A$ (k volte)\n",
    "\n",
    "# Sistemi lineari\n",
    "\n",
    "**Proposizione** Sia dato un sistema lineare di 2 equazioni in 2 incognite $AX=B$. Se $\\text{det}A=ad-bc\\neq 0$, allora l'unica soluzione del sistema è $X=A^{-1}B$\n",
    "\n",
    "## Metodo di eliminazione di Gauss\n",
    "\n",
    "Dato un sistema lineare\n",
    "\n",
    "$$\\left\\{\\begin{matrix}ax+by=\\alpha\\\\ cx+dy=\\beta\\end{matrix}\\right.$$\n",
    "\n",
    "1. Si scrive la matrice completa associata al sistema lineare\n",
    "\n",
    "$$A\\mid B=\\left[{\\begin{array}{cc|c}a & b & \\alpha\\\\c & d & \\beta\\end{array}}\\right]$$\n",
    "\n",
    "2. Si riduce completamente la matrice completa\n",
    "\n",
    "$$A\\mid B=\\left[{\\begin{array}{cc|c}1 & 0 & \\gamma\\\\0 & 1 & \\delta\\end{array}}\\right]$$\n",
    "\n",
    "3. Si scrive il sistema associato alla matrice completa completamente ridotta\n",
    "\n",
    "$$\\left\\{\\begin{matrix}x=\\gamma\\\\ y=\\delta\\end{matrix}\\right.$$\n",
    "\n",
    "**Osservazione** Se nella matrice completa completamente ridotta compare un pivot nell'ultima colonna, e solo in questo caso, allora il sistema è irrisolubile (o incompatibile).\n",
    "\n",
    "## Spazio delle soluzioni di un sistema lineare\n",
    "\n",
    "**Teorema** (di Rouché-Capelli) Un sistema lineare $AX=B$ ammette soluzioni se e solo se $\\text{r}(A)=\\text{r}(A\\mid B)$.\n",
    "\n",
    "**Teorema** Sia $AX=B$ un sistema lineare. Le soluzioni di $AX=B$ si possono ottenere sommando tutte le soluzioni del sistema lineare omogeneo associato $AX=0$ a una soluzione $X_0$ di $AX=B$.\n",
    "\n",
    "**Dimostrazione** Prendiamo una soluzione, ammesso che esista, $X_0$. Allora $AX_0=B$. Se prendiamo un'altra soluzione $X$ abbiamo $AX=B$. Sottraendo la prima dalla seconda otteniamo $A(X-X_0)=0$. Dunque $X=(X-X_0)+X_0$, dove $X-X_0$ è una soluzione del sistema omogeneo associato.\n",
    "\n",
    "**Teorema** Se esistono soluzioni, la dimensione dello spazio delle soluzioni è data da $n-\\text{r}(A)$, dove $n$ è il numero di incognite e $A$ è la matrice completa (o incompleta).\n",
    "\n",
    "**Osservazione** A volte si definisce la dimensione dello spazio delle soluzioni di un sistema irrisolubile come $-1$.\n",
    "\n",
    "**Proposizione** Un sistema lineare può avere solo $0$ o $1$ o $\\infty$ soluzioni.\n",
    "\n",
    "# Applicazioni lineari (parte 1)\n",
    "\n",
    "**Definizione** Siano $V,W$ spazi vettoriali e $f:V\\rightarrow W$ una funzione. $f$ si dice **applicazione lineare** (o **trasformazione lineare**) se:\n",
    "\n",
    "1. $\\forall v_1,v_2\\in V,\\ \\ \\ \\ f(v_1+v_2)=f(v_1)+f(v_2)$\n",
    "2. $\\forall v\\in V,\\ \\forall \\lambda\\in\\mathbb{R},\\ \\ \\ \\ f(\\lambda v)=\\lambda f(v)$\n",
    "\n",
    "**Definizione** Sia $T:V\\rightarrow W$ una trasformazione lineare.\n",
    "\n",
    "1. Si definisce **immagine** di T\n",
    "\n",
    "$$\\text{Im}T=\\{w\\in W\\mid \\exists v\\in V:\\ T(v)=w\\}$$\n",
    "\n",
    "2. Si definisce **nucleo** di T\n",
    "\n",
    "$$\\text{ker}T=\\{v\\in V\\mid T(v)=0_W\\}$$\n",
    "\n",
    "**Osservazione** La $T:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ è lineare se ogni sua componente si può scrivere come un polinomio omogeneo di primo grado o come il polinomio nullo.\n",
    "\n",
    "**Proposizione** $\\text{Im}T$ e $\\text{ker}T$ sono sempre sottospazi vettoriali rispettivamente di $W$ e $V$.\n",
    "\n",
    "**Dimostrazione**\n",
    "\n",
    "1. Siano $T(v_1),T(v_2)\\in\\text{Im}T$ e $\\lambda\\in\\mathbb{R}$\n",
    "\n",
    "$$T(v_1)+T(v_2)=T(v_1+v_2)\\in\\text{Im}T\\ \\text{(per linearità)}$$\n",
    "\n",
    "$$T(\\lambda v) = \\lambda T(v)\\in\\text{Im}T\\ \\text{(per linearità)}$$\n",
    "\n",
    "2. Siano $v1,v2\\in\\text{ker}T\\iff T(v_1)=T(v_2)=0_W$ e $\\lambda\\in\\mathbb{R}$\n",
    "\n",
    "$$T(v_1+v_2)=T(v_1)+T(v_2)=0_W$$\n",
    "\n",
    "$$T(\\lambda v)=\\lambda T(v)=0_W$$\n",
    "\n",
    "**Proposizione** Sia $T:V\\rightarrow W$ una trasformazione lineare.\n",
    "\n",
    "1. $T$ è suriettiva se e solo se $\\text{Im}T$\n",
    "2. $T$ è iniettiva se e solo se $\\text{ker}T=\\{0_V\\}$\n",
    "\n",
    "**Teorema** (Equazione dimensionale) Sia $T:V\\rightarrow W$ una trasformazione lineare ($\\text{dim}V, \\text{dim}W < -\\infty$).\n",
    "\n",
    "$$\\text{dim}V=\\text{dim}(\\text{ker}T)+\\text{dim}(\\text{Im}T)$$\n",
    "\n",
    "**Dimostrazione** Prendiamo una base $B_k=\\{v_1,...,v_r\\}$ di $\\text{ker}T$. Completiamola a una base $B_V=\\{v_1,...,v_r,v_{r+1},...,v_n\\}$. Dimostriamo che $S=\\{T(v_{r+1}),...,T(v_n)\\}$ è una base per $\\text{Im}T$.\n",
    "\n",
    "1. Prendiamo $T(v)$, con $v\\in V$ e scriviamolo come\n",
    "\n",
    "$$T(v)=T(x_1v_1+...+x_rv_r+x_{r+1}v_{r+1}+...+x_nv_n)=$$\n",
    "\n",
    "$$=x_1T(v_1)+...+x_rT(v_r)+x_{r+1}T(v_{r+1})+...+x_nT(v_n)$$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Siccome $T(v_1)=...=T(v_r)=0_W$ perché immagini di vettori appartenenti al nucleo di T\n",
    "\n",
    "$$T(v)=x_{r+1}T(v_{r+1})+...+x_nT(v_n)$$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Quindi $S$ è un sistema di generatori per $\\text{Im}T$.\n",
    "\n",
    "2. Dimostriamo che $x_{r+1}T(v_{r+1})+...+x_nT(v_n)=0_W\\iff x_1=...=x_n=0$. Per linearità:\n",
    "\n",
    "$$T(x_{r+1}v_{r+1}+...+x_nv_n)=0_W$$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Il vettore $x_{r+1}v_{r+1}+...+x_nv_n$ si può scrivere come combinazione lineare dei vettori di $B_k$:\n",
    "\n",
    "$$x_{r+1}v_{r+1}+...+x_nv_n=\\alpha_1v_1+...+\\alpha_rv_r$$\n",
    "\n",
    "$$\\alpha_1v_1+...+\\alpha_rv_r-x_{r+1}v_{r+1}-...+x_nv_n=0_V$$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;Siccome $v_1,...,v_r,v_{r+1},...,v_n$ formano la base $B_V$, allora sono linearmente indipendenti, quindi\n",
    "\n",
    "$$x_{r+1}=...=x_n=\\alpha_1=...=\\alpha_r=0$$\n",
    "\n",
    "Abbiamo quindi $\\text{dim}V=n+r$, $\\text{dim}(\\text{ker}T)=r$ e $\\text{dim}(\\text{Im}T)=n$, che soddisfano la tesi.\n",
    "\n",
    "**Proposizione** Sia $T:V\\rightarrow W$ una trasformazione lineare, $n=\\text{dim}V$, $m=\\text{dim}W$. Allora:\n",
    "\n",
    "1. se $n<m$, $T$ non può essere suriettiva\n",
    "2. se $n>m$, $T$ non può essere iniettiva\n",
    "\n",
    "**Dimostrazione**\n",
    "\n",
    "1. Dimostriamo la contronominale. $T$ suriettiva implica $\\text{dim}(\\text{Im}T)=m$. Sappiamo inoltre che $\\text{dim}(\\text{ker}T)\\geq 0$. Usando l'equazione dimensionale, necessariamente $n\\geq m$.\n",
    "\n",
    "2. Dimostriamo la contronominale. $T$ iniettiva implica $\\text{dim}(\\text{ker}T)=0$. Usando l'equazione dimensionale abbiamo $\\text{dim}(\\text{Im}T)=n$. Sappiamo inoltre che $m\\geq\\text{dim}(\\text{Im}T)=n$.\n",
    "\n",
    "**Teorema** Sia $T:V^n\\rightarrow W^m$ una trasformazione lineare e $n=m$. Allora\n",
    "\n",
    "$$T\\ \\text{è iniettiva}\\iff T\\ \\text{è suriettiva}$$\n",
    "\n",
    "**Dimostrazione** Si dimostrano entrambe le implicazioni facendo uso dell'equazione dimensionale\n",
    "\n",
    "($\\implies$) $\\text{dim}(\\text{ker}T)=0\\implies \\text{dim}(\\text{Im}T)=n=m$\n",
    "\n",
    "($\\impliedby$) $\\text{dim}(\\text{Im}T)=m\\implies \\text{dim}(\\text{ker}T)=0$\n",
    "\n",
    "## Isomorfismi\n",
    "\n",
    "**Definizione** Si definisce **isomorfismo** una trasformazione lineare $f:V\\rightarrow W$ biunivoca.\n",
    "\n",
    "**Definizione** Sia $f:V\\rightarrow W$ un isomorfismo. Allora $\\exists f^{-1}:W\\rightarrow V$ tale che\n",
    "\n",
    "1. $f^{-1}(w_1+w_2)=f^{-1}(w_1)+f^{-1}(w_2)$\n",
    "2. $f^{-1}(\\lambda w)=\\lambda f^{-1}(w)$\n",
    "\n",
    "**Dimostrazione**\n",
    "\n",
    "1. $f^{-1}(w_1+w_2)=f^{-1}(f(v_1+v_2))=v_1+v_2=f^{-1}(w_1)+f^{-1}(w_2)$\n",
    "2. $f^{-1}(\\lambda w)=f^{-1}(\\lambda f(v))=f^{-1}(f(\\lambda v))=\\lambda v=\\lambda f^{-1}(w)$\n",
    "\n",
    "**Teorema** Ogni spazio vettoriale $V$ con $\\text{dim}V=n$ è isomorfo a $\\mathbb{R}^n$.\n",
    "\n",
    "\n",
    "# Matrici e applicazioni lineari\n",
    "\n",
    "**Proposizione** Sia $T:V\\rightarrow W$ una trasformazione lineare e siano $V,W$ spazi vettoriali di dimensioni finite $\\text{dim}V=n$ e $\\text{dim}W=m$. Date due basi qualunque $B_V=\\{v_1,...,v_n\\}$ e $B_W=\\{w_1,...,w_m\\}$ rispettivamente di $V$ e $W$, se $\\forall j=1,...,n$ si ha $f(v_j)=a_i^jw_1+...+a_m^jw_m$, allora la matrice associata a $T$ rispetto alle basi $B_V$ e $B_W$, $M_{B_VB_W}(T)=(a_i^j)\\in M_{m\\times n}(\\mathbb{R})$ è tale che, $\\forall v\\in V$, il vettore $f(v)\\in W$ ha coordinate rispetto a $B_W$\n",
    "\n",
    "$$\\begin{bmatrix}y_1\\\\ ...\\\\ y_m\\end{bmatrix}=M_{B_VB_W}(T)\\begin{bmatrix}x_1\\\\ ...\\\\ x_n\\end{bmatrix}$$\n",
    "\n",
    "dove $x_1,...,x_n$ sono le coordinate di $v$ rispetto a $B_V$.\n",
    "\n",
    "**Dimostrazione** Sia $T:V\\rightarrow W$ una trasformazione lineare. Sia $B_V$ una base di $V$ e $B_W$ una base di $W$. Consideriamo un vettore $v\\in V$ e la sua immagine $w=T(v)$.\n",
    "\n",
    "$$B_V=\\{v_1,...,v_n\\}\\ \\ \\ \\ B_W=\\{w_1,...,w_m\\}$$\n",
    "\n",
    "$$v\\equiv_{B_V}(x_1,...,x_n)\\ \\ \\ \\ v\\equiv_{B_W}(y_1,...,y_m)$$\n",
    "\n",
    "Calcoliamo $T(v)$\n",
    "\n",
    "$$T(v)=T(x_1v_1+...+x_nv_n)=x_1T(v_1)+...+x_nT(v_n)$$\n",
    "\n",
    "Possiamo riscrivere i vettori $T(v_1),...,T(v_n)$ come:\n",
    "\n",
    "$$T(v_1)=a_1^1w_1+a_2^1w_2+...+a_m^1w_m$$\n",
    "$$...$$\n",
    "$$T(v_n)=a_1^nw_1+a_2^nw_2+...+a_m^nw_m$$\n",
    "\n",
    "Quindi\n",
    "\n",
    "$$x_1(a_1^1w_1+a_2^1w_2+...+a_m^1w_m)+...+x_n(a_1^nw_1+a_2^nw_2+...+a_m^nw_m)=$$\n",
    "$$=w_1(a_1^1x_1+...+a_1^nx_n)+...+w_m(a_m^1x_1+...+a_m^nx_n)$$\n",
    "\n",
    "Poniamo\n",
    "\n",
    "$$y_1=a_1^1x_1+...+a_1^nx_n$$\n",
    "$$...$$\n",
    "$$y_m=a_m^1x_1+...+a_m^nx_n$$\n",
    "\n",
    "Possiamo quindi scrivere i coefficienti $a_1^1,...,a_m^n$ come una matrice\n",
    "\n",
    "$$A=\\begin{bmatrix}a_1^1 & ... & a_1^n\\\\ ... & ... & ...\\\\ a_m^1 & ... & a_m^n\\end{bmatrix}$$\n",
    "\n",
    "e le coordinate di $v$ rispetto a $B_V$ come un vettore colonna\n",
    "\n",
    "$$\\begin{bmatrix}x_1\\\\ ...\\\\ x_n\\end{bmatrix}$$\n",
    "\n",
    "Perciò\n",
    "\n",
    "$$\\begin{bmatrix}y_1\\\\ ...\\\\ y_m\\end{bmatrix}=A\\begin{bmatrix}x_1\\\\ ...\\\\ x_n\\end{bmatrix}=\\begin{bmatrix}a_1^1 & ... & a_1^n\\\\ ... & ... & ...\\\\ a_m^1 & ... & a_m^n\\end{bmatrix}\\begin{bmatrix}x_1\\\\ ...\\\\ x_n\\end{bmatrix}$$\n",
    "\n",
    "**Proposizione** Sia $T:V\\rightarrow W$ trasformazione lineare e $\\text{dim}V=n$. Sia $A=M_{m\\times n}(T)$ la matrice associata a $T$  rispetto alle basi $B_V$ e $B_W$. Allora\n",
    "\n",
    "1. $\\text{dim}(\\text{Im}T)=\\text{r}(A)$\n",
    "2. $\\text{dim}(\\text{ker}T)=n-\\text{r}(A)$\n",
    "\n",
    "**Osservazione** Prendiamo il seguente caso:\n",
    "\n",
    "$$\\underset{base B}{V^n}\\overset{f}{\\longrightarrow}\\underset{base C}{W^m}\\overset{g}{\\longrightarrow}\\underset{base D}{U^k}$$\n",
    "\n",
    "Vogliamo adesso trovare la matrice associata a $g\\circ f$ rispetto alle basi $B$ e $D$.\n",
    "\n",
    "$$\\underset{base B}{V^n}\\overset{g\\circ f}{\\longrightarrow}\\underset{base D}{U^k}$$\n",
    "\n",
    "Sappiamo che:\n",
    "\n",
    "$$x=\\begin{bmatrix}x_1\\\\ ...\\\\ x_n\\end{bmatrix}\\ \\ \\ \\ y=\\begin{bmatrix}y_1\\\\ ...\\\\ y_m\\end{bmatrix} \\ \\ \\ \\ z=\\begin{bmatrix}z_1\\\\ ...\\\\ z_k\\end{bmatrix}$$\n",
    "\n",
    "sono i vettori delle coordinate rispettivamente di $v$, $f(v)$ e $g(f(v))$. Sappiamo inoltre\n",
    "\n",
    "$$y=M_{BC}(f)x\\ \\ \\ \\ z=M_{CD}(g)y=M_{CD}(g)M_{BC}(f)y$$\n",
    "\n",
    "Quindi la matrice associata a $g\\circ f$ rispetto alle basi $B$ e $D$ è:\n",
    "\n",
    "$$M_{BD}(g\\circ f)=M_{CD}(g)M_{BC}(f)$$\n",
    "\n",
    "## Matrice del cambiamento di base\n",
    "\n",
    "**Definizione** Data\n",
    "\n",
    "$$f:\\underset{base B}{V}\\overset{id_V}{\\longrightarrow}\\underset{base B'}{V}$$\n",
    "\n",
    "$M_{BB'}(id_V)$ è detta **matrice del cambiamento di base** da $B$ a $B'$.\n",
    "\n",
    "**Osservazione** Prendiamo il seguente caso:\n",
    "\n",
    "$$f:\\underset{base B}{V}\\overset{id_V}{\\longrightarrow}\\underset{base B'}{V}\\overset{id_V}{\\longrightarrow}\\underset{base B}{V}$$\n",
    "\n",
    "La matrice associata alla trasformazione composta $id_V\\circ id_V$ è $M_{BB}(id_V)$, che è la matrice identità. Questo significa\n",
    "\n",
    "$$M_{BB'}(id)=(M_{B'B}(id))^{-1}$$\n",
    "\n",
    "**Osservazione** Come cambia la matrice del cambiamento di base quando cambia la base di $V$?\n",
    "\n",
    "$$\\underset{base B'}{V}\\overset{id_V}{\\longrightarrow}\\underset{base B}{V}\\overset{f}{\\longrightarrow}\\underset{base B}{V}\\overset{id_V}{\\longrightarrow}\\underset{base B'}{V}$$\n",
    "\n",
    "$$M_{B'B'}(id_V\\circ f\\circ id_V)=M_{BB'}(id_V)M_{BB}(f)M_{B'B}(id_V)$$\n",
    "\n",
    "# Applicazioni lineari (parte 2)\n",
    "\n",
    "**Teorema** (fondamentale delle trasformazioni lineari) Siano $V$ e $W$ due spazi vettoriali finitamente generati. Siano $B=(v_1,...,v_n)$ una base di $V$ e $\\{w_1,...,w_n\\}$ una n-upla di $W$. Esiste una e una sola trasformazione lineare $T:V\\rightarrow W$ tale che $T(v_1)=w_1,...,T(v_n)=w_n$.\n",
    "\n",
    "**Dimostrazione** Supponiamo che $T$ esista. Prendiamo $v\\in V$ tale che $v=x_1v_1+...+x_nv_n$.\n",
    "\n",
    "$$T(v)=T(x_1v_1+...+x_nv_n)=x_1T(v_1)+...+x_nT(v_n)=x_1w_1+...+x_nw_n$$\n",
    "\n",
    "L'esistenza di $T$ deriva dall'osservazione che la funzione $x_1v_1+...+x_nv_n\\rightarrow x_1w_1+...+x_nw_n$ è lineare.\n",
    "\n",
    "**Definizione** Un'applicazione lineare $T:V\\rightarrow V$ è detta **endomorfismo**.\n",
    "\n",
    "**Definizione** Un endomorfismo biunivoco è detto **automorfismo**\n",
    "\n",
    "**Proposizione** $A\\in M_{n\\times n}(\\mathbb{K})$ è invertibile se e solo se $\\text{r}(A)=n$.\n",
    "\n",
    "**Dimostrazione** Associamo $A$ a una trasformazione lineare $T:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n$ e sia $\\mathcal{E}_n$ la base canonica.\n",
    "\n",
    "($\\implies$) $T$ è invertibile se e solo se è biunivoca, ovvero se è iniettiva e suriettiva. Sappiamo che $T$ è suriettiva se e solo se $\\text{dim\\ Im}T=n$ e sappiamo che $\\text{dim\\ Im}T=\\text{r}(A)$, quindi $\\text{r}(A)=n$.\n",
    "\n",
    "($\\impliedby$) Sappiamo che $\\text{r}(A)=\\text{dim\\ Im}T$, quindi $\\text{dim\\ Im}T = n$. Perciò $T$ è suriettiva e iniettiva, quindi biunivoca e invertibile.\n",
    "\n",
    "**Definizione** Due matrici $A,B\\in M_{n\\times n}(\\mathbb{K})$ si dicono **simili** se esiste una matrice $P$ invertibile tale che $A=PBP^{-1}$\n",
    "\n",
    "**Osservazione** La similitudine è una relazione di equivalenza:\n",
    "\n",
    "1. **riflessiva** $A=PAP^{-1}$\n",
    "2. **simmetrica** $B=PAP^{-1}\\implies A=P^{-1}BP$\n",
    "3. **transitiva** $B=PAP^{-1}$ e $C=QBQ^{-1}$ $\\implies$ $C=(QP)A(QP)^{-1}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
